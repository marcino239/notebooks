{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Andriej Karpathy wrote great [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness) on teaching a Recursive Neural Net structure of the english language.  Let's try to run few experiments based on the minimal implementation of the RNN algorithm.  The code for the experiments is in [this git](https://github.com/marcino239/min_rnn).\n",
      "\n",
      "An RNN cell is described by the following equations:\n",
      "\n",
      "$$ h_{in} = W_{xh} x + W_{hh} h + b_{h} $$\n",
      "$$ h' = f( h_{in} ) $$\n",
      "$$ y = W_{xh} h' + b_{y} $$\n",
      "\n",
      "And with softmax output:\n",
      "\n",
      "$$ p_i = \\frac{ e^{y_i} }{ \\sum{y_i} } $$\n",
      "\n",
      "Original implementation of the RNN uses tanh as an activation function $ f( h_{in} ) $.  The backprogation through tanh therefore is:\n",
      "\n",
      "$$ dh_{raw} = (1 - \\tanh^2( h_{in} )) dh' $$\n",
      "\n",
      "I was curious how Rectified Linear Unit (RELU) would perform during text recovery via RNN. The forward pass in case of RELU is trivial:\n",
      "\n",
      "$$ h' = \\max( 0, W_{xh} x + W_{hh} h + b_{h} ) $$\n",
      "\n",
      "The backward pass is not too complicated either:\n",
      "\n",
      "$$ dh_{raw} = sig( h_{in} ) dh $$\n",
      "\n",
      "\n",
      "The output from the two comparable nets looks quite interesting (the net was trained on Paul Graham essays).  This is what we get after 2,078,400 iterations from tanh version:  \n",
      "\n",
      "_\n",
      " he reems and gens Alance your fing dation such in is or dereacted to next if you've about on surpricate shof you've work like that the moypravisy 5 9-be idess Leforter, because is that atered as they  \n",
      "_\n",
      "\n",
      "A bit bubly isn't it?  Here's the RELU version after the same number of iterations:  \n",
      "\n",
      "_\n",
      " hat it found lices like. But language job sharen relras answeaking happens and by business could want you explane-- not just thacker durery is decide the do you be meaged to figure finiction it don't  \n",
      "_\n",
      "  \n",
      "Still bubly, albeit a bit less.  The following chart shows loss as a function of iterations.  \n",
      "<img src=\"files/images/rnn_loss_tanh_relu.png\" />"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}