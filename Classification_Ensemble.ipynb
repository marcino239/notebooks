{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In previous posts we've looked at single classifiers in action.  Let's asses in this post a voting class of classifiers.  \n",
      "Logistic regression comes first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import linear_model, datasets\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "digits = datasets.load_digits()\n",
      "\n",
      "# split dataset into train and test\n",
      "X_train, X_test, y_train, y_test = train_test_split( digits.data, \n",
      "                                                    digits.target,\n",
      "                                                    test_size=0.33 )\n",
      "Cs = np.logspace(-4., 4., 20)\n",
      "\n",
      "logreg_cv = linear_model.LogisticRegressionCV( Cs=Cs )\n",
      "logreg_cv.fit( X_train, y_train )\n",
      "\n",
      "print( 'Logistic Regression: C:{0}, score:{1}'.format( np.average( logreg_cv.C_ ),\n",
      "                            logreg_cv.score( X_train, y_train ) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Logistic Regression: C:3.05929854461, score:0.991687448047\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will set up KNN classifier for the train set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, neighbors\n",
      "\n",
      "# use default number of neighbors (5)\n",
      "nbor = neighbors.KNeighborsClassifier()\n",
      "\n",
      "# fit the predictor\n",
      "nbor.fit( X_train, y_train )\n",
      "\n",
      "print( 'KNN Test score:{0}'.format( nbor.score( X_train, y_train ) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "KNN Test score:0.995012468828\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last one will be SVM classifier with linear kernel.  We are going to use a \"bad\" value for regularization parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "\n",
      "# we will use default value for C here for a moment\n",
      "clf_svm = svm.LinearSVC(penalty='l1', loss='squared_hinge',\n",
      "                    dual=False, tol=1e-3, C=1e-3 )\n",
      "clf_svm.fit( X_train, y_train )\n",
      "\n",
      "print( 'Lin SVC: Test score:{0}'.format( clf_svm.score( X_train, y_train ) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lin SVC: Test score:0.896924355777\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's define a voting classifier.  The code is copied from sklearn source for 0.17.0 and credit goes to Sebastian Raschka @rasbt.  \n",
      "\n",
      "Comments and docstrings are removed for readability :)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "\n",
      "from sklearn.base import BaseEstimator\n",
      "from sklearn.base import ClassifierMixin\n",
      "from sklearn.base import TransformerMixin\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.externals import six\n",
      "from sklearn.base import clone\n",
      "from sklearn.pipeline import _name_estimators\n",
      "\n",
      "class VotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
      "    def __init__(self, clfs, voting='hard', weights=None):\n",
      "        self.clfs = clfs\n",
      "        self.named_clfs = {key:value for key,value in _name_estimators(clfs)}\n",
      "        self.voting = voting\n",
      "        self.weights = weights\n",
      "\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
      "            raise NotImplementedError('Multilabel and multi-output'\\\n",
      "                                      ' classification is not supported.')\n",
      "\n",
      "        if self.voting not in ('soft', 'hard'):\n",
      "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
      "                             % voting)\n",
      "\n",
      "        if self.weights and len(self.weights) != len(self.clfs):\n",
      "            raise ValueError('Number of classifiers and weights must be equal'\n",
      "                             '; got %d weights, %d clfs'\n",
      "                             % (len(self.weights), len(self.clfs)))\n",
      "\n",
      "        self.le_ = LabelEncoder()\n",
      "        self.le_.fit(y)\n",
      "        self.classes_ = self.le_.classes_\n",
      "        self.clfs_ = []\n",
      "        for clf in self.clfs:\n",
      "            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\n",
      "            self.clfs_.append(fitted_clf)\n",
      "        return self\n",
      "\n",
      "    def predict(self, X):\n",
      "        if self.voting == 'soft':\n",
      "\n",
      "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
      "\n",
      "        else:  # 'hard' voting\n",
      "            predictions = self._predict(X)\n",
      "\n",
      "            maj = np.apply_along_axis(\n",
      "                                      lambda x:\n",
      "                                      np.argmax(np.bincount(x,\n",
      "                                                weights=self.weights)),\n",
      "                                      axis=1,\n",
      "                                      arr=predictions)\n",
      "\n",
      "        maj = self.le_.inverse_transform(maj)\n",
      "        return maj\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        avg = np.average(self._predict_probas(X), axis=0, weights=self.weights)\n",
      "        return avg\n",
      "\n",
      "    def transform(self, X):\n",
      "        if self.voting == 'soft':\n",
      "            return self._predict_probas(X)\n",
      "        else:\n",
      "            return self._predict(X)\n",
      "\n",
      "    def get_params(self, deep=True):\n",
      "        if not deep:\n",
      "            return super(EnsembleClassifier, self).get_params(deep=False)\n",
      "        else:\n",
      "            out = self.named_clfs.copy()\n",
      "            for name, step in six.iteritems(self.named_clfs):\n",
      "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
      "                    out['%s__%s' % (name, key)] = value\n",
      "            return out\n",
      "\n",
      "    def _predict(self, X):\n",
      "        return np.asarray([clf.predict(X) for clf in self.clfs]).T\n",
      "\n",
      "    def _predict_probas(self, X):\n",
      "        return np.asarray([clf.predict_proba(X) for clf in self.clfs])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's put all these together using VotingClassifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimators = ( logreg_cv, nbor, clf_svm )\n",
      "vclf = VotingClassifier( clfs=estimators,\n",
      "                        voting='hard',\n",
      "                        weights=[1, 1, 1])\n",
      "\n",
      "vclf.fit( X_train, y_train )\n",
      "\n",
      "for e in _name_estimators( estimators ):\n",
      "    print( '{0} score on test data: {1}'.format( e[0], e[1].score( X_test, y_test ) ) )\n",
      "\n",
      "# final test\n",
      "print( '\\n' )\n",
      "print( 'hard voting: Test score:{0}'.format( vclf.score( X_test, y_test ) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "logisticregressioncv score on test data: 0.939393939394\n",
        "kneighborsclassifier score on test data: 0.974747474747"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "linearsvc score on test data: 0.877104377104\n",
        "\n",
        "\n",
        "hard voting: Test score:0.951178451178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}